{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d1570-c29e-4c0e-b830-4b7a41bc3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n_observations = 1200 \n",
    "n_features = 4        \n",
    "\n",
    "np.random.seed(42)\n",
    "base_series = np.cumsum(np.random.normal(0, 1, n_observations))\n",
    "\n",
    "data_dict = {'feature_1': base_series}\n",
    "for i in range(1, n_features):\n",
    "    noise = np.random.normal(0, 0.5, n_observations)\n",
    "    correlated_series = base_series + noise * (1 + i * 0.1)\n",
    "    data_dict[f'feature_{i+1}'] = correlated_series\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "observations_met = df.shape[0] >= 1000\n",
    "features_met = df.shape[1] >= 3\n",
    "\n",
    "print(f\"Observations >= 1000: {observations_met}\")\n",
    "print(f\"Features >= 3: {features_met}\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "print(\"First 5 rows of scaled dataset:\")\n",
    "print(df_scaled.head())\n",
    "\n",
    "df_differenced = df_scaled.diff().dropna()\n",
    "print(\"\\nFirst 5 rows of differenced dataset:\")\n",
    "print(df_differenced.head())\n",
    "\n",
    "print(f\"\\nShape of differenced dataset: {df_differenced.shape}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_sequences(data, look_back):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:(i + look_back)])\n",
    "        y.append(data[i + look_back])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 20\n",
    "\n",
    "data_array = df_differenced.values\n",
    "\n",
    "X, y = create_sequences(data_array, look_back)\n",
    "\n",
    "print(f\"Shape of X (sequences): {X.shape}\")\n",
    "print(f\"Shape of y (targets): {y.shape}\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "total_samples = len(X)\n",
    "train_samples = int(total_samples * train_ratio)\n",
    "val_samples = int(total_samples * val_ratio)\n",
    "\n",
    "X_train = X[:train_samples]\n",
    "y_train = y[:train_samples]\n",
    "\n",
    "X_val = X[train_samples : train_samples + val_samples]\n",
    "y_val = y[train_samples : train_samples + val_samples]\n",
    "\n",
    "X_test = X[train_samples + val_samples :]\n",
    "y_test = y[train_samples + val_samples :]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "print(\"TensorFlow and Keras components imported successfully.\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "n_features = X_train.shape[2] \n",
    "\n",
    "model_lstm = Sequential([\n",
    "    tf.keras.Input(shape=(look_back, n_features)), \n",
    "    LSTM(units=50, activation='relu'),\n",
    "    Dense(units=n_features)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(model_lstm.summary())\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(\"Keras functional API components imported successfully.\")\n",
    "\n",
    "encoder_units = 64\n",
    "\n",
    "encoder_inputs = Input(shape=(look_back, n_features))\n",
    "encoder_lstm = LSTM(encoder_units, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(1, n_features)) \n",
    "decoder_lstm = LSTM(encoder_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(encoder_units,))\n",
    "decoder_state_input_c = Input(shape=(encoder_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h_decoder, state_c_decoder = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_decoder, state_c_decoder]\n",
    "\n",
    "decoder_dense = Dense(n_features)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "print(\"Encoder Model Summary:\")\n",
    "print(encoder_model.summary())\n",
    "\n",
    "print(\"\\nDecoder Model Summary:\")\n",
    "print(decoder_model.summary())\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, AdditiveAttention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "decoder_inputs_attn = Input(shape=(1, n_features), name='decoder_input_attention') \n",
    "decoder_state_input_h = Input(shape=(encoder_units,), name='decoder_state_input_h')\n",
    "decoder_state_input_c = Input(shape=(encoder_units,), name='decoder_state_input_c')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_layer = LSTM(encoder_units, return_sequences=True, return_state=True, name='decoder_lstm_layer')\n",
    "decoder_lstm_outputs, decoder_state_h, decoder_state_c = decoder_lstm_layer(\n",
    "    decoder_inputs_attn, initial_state=decoder_states_inputs\n",
    ")\n",
    "\n",
    "attention_layer = AdditiveAttention(name='bahdanau_attention')\n",
    "attention_output = attention_layer([decoder_lstm_outputs, encoder_outputs])\n",
    "\n",
    "decoder_concat_input = Concatenate(axis=-1, name='decoder_output_and_attention')([decoder_lstm_outputs, attention_output])\n",
    "\n",
    "decoder_dense_layer = Dense(n_features, name='decoder_output_dense_layer')\n",
    "decoder_outputs = decoder_dense_layer(decoder_concat_input)\n",
    "\n",
    "model_attention_seq2seq = Model(\n",
    "    [encoder_inputs, decoder_inputs_attn, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs, decoder_state_h, decoder_state_c]\n",
    ")\n",
    "\n",
    "model_attention_seq2seq.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(\"\\nAttention-based Seq2Seq Model Summary:\")\n",
    "print(model_attention_seq2seq.summary())\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "print(\"\\nTraining Baseline LSTM Model...\")\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Baseline LSTM Model Training Complete.\")\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32 \n",
    "\n",
    "print(\"\\nPreparing inputs for Attention-based Seq2Seq Model...\")\n",
    "\n",
    "encoder_h_train, encoder_c_train = encoder_model.predict(X_train)\n",
    "\n",
    "encoder_h_val, encoder_c_val = encoder_model.predict(X_val)\n",
    "\n",
    "decoder_input_train = y_train.reshape(y_train.shape[0], 1, y_train.shape[1])\n",
    "\n",
    "decoder_input_val = y_val.reshape(y_val.shape[0], 1, y_val.shape[1])\n",
    "\n",
    "print(f\"encoder_h_train shape: {encoder_h_train.shape}\")\n",
    "print(f\"encoder_c_train shape: {encoder_c_train.shape}\")\n",
    "print(f\"decoder_input_train shape: {decoder_input_train.shape}\")\n",
    "\n",
    "print(\"Inputs preparation complete.\")\n",
    "\n",
    "print(\"\\nTraining Attention-based Seq2Seq Model...\")\n",
    "\n",
    "history_attention = model_attention_seq2seq.fit(\n",
    "    [X_train, decoder_input_train, encoder_h_train, encoder_c_train],\n",
    "    [y_train, encoder_h_train, encoder_c_train], \n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=([\n",
    "        X_val, decoder_input_val, encoder_h_val, encoder_c_val\n",
    "    ],\n",
    "    [y_val, encoder_h_val, encoder_c_val]\n",
    "    ),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Attention-based Seq2Seq Model Training Complete.\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, AdditiveAttention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(\"Generating predictions for Baseline LSTM Model...\")\n",
    "lstm_predictions = model_lstm.predict(X_test, verbose=0)\n",
    "print(\"Baseline LSTM Model predictions generated.\")\n",
    "print(\"Setting up inference models for Attention-based Seq2Seq Model...\")\n",
    "\n",
    "encoder_inputs_inf = Input(shape=(look_back, n_features), name='encoder_inputs_inference_graph')\n",
    "encoder_outputs_inf, state_h_inf, state_c_inf = encoder_lstm(encoder_inputs_inf) \n",
    "encoder_inference_model = Model(encoder_inputs_inf, [encoder_outputs_inf, state_h_inf, state_c_inf])\n",
    "decoder_input_single_step = Input(shape=(1, n_features), name='decoder_input_single_step')\n",
    "decoder_h_input = Input(shape=(encoder_units,), name='decoder_h_input')\n",
    "decoder_c_input = Input(shape=(encoder_units,), name='decoder_c_input')\n",
    "decoder_encoder_outputs_input = Input(shape=(look_back, encoder_units), name='decoder_encoder_outputs_input') \n",
    "\n",
    "decoder_lstm_outputs_inf, new_h, new_c = decoder_lstm_layer(\n",
    "    decoder_input_single_step, initial_state=[decoder_h_input, decoder_c_input]\n",
    ")\n",
    "\n",
    "attention_output_inf = attention_layer([decoder_lstm_outputs_inf, decoder_encoder_outputs_input])\n",
    "decoder_concat_input_inf = Concatenate(axis=-1, name='decoder_output_and_attention_inference')([decoder_lstm_outputs_inf, attention_output_inf])\n",
    "decoder_outputs_inf = decoder_dense_layer(decoder_concat_input_inf)\n",
    "\n",
    "decoder_inference_model = Model(\n",
    "    [decoder_input_single_step, decoder_h_input, decoder_c_input, decoder_encoder_outputs_input],\n",
    "    [decoder_outputs_inf, new_h, new_c]\n",
    ")\n",
    "\n",
    "print(\"Attention-based Seq2Seq Inference models created.\")\n",
    "\n",
    "attention_predictions = []\n",
    "\n",
    "print(\"Generating predictions for Attention-based Seq2Seq Model...\")\n",
    "for i in range(len(X_test)):\n",
    "    current_X_test_sample = X_test[i:i+1]\n",
    "    encoder_outputs_test, h_state, c_state = encoder_inference_model.predict(current_X_test_sample, verbose=0)\n",
    "    decoder_input_data = np.zeros((1, 1, n_features))\n",
    "    output_prediction, h_state, c_state = decoder_inference_model.predict(\n",
    "        [decoder_input_data, h_state, c_state, encoder_outputs_test], verbose=0\n",
    "    )\n",
    "    attention_predictions.append(output_prediction.squeeze())\n",
    "\n",
    "attention_predictions = np.array(attention_predictions)\n",
    "print(\"Attention-based Seq2Seq Model predictions generated.\")\n",
    "print(f\"Shape of LSTM predictions: {lstm_predictions.shape}\")\n",
    "print(f\"Shape of Attention-based Seq2Seq predictions: {attention_predictions.shape}\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test, lstm_predictions))\n",
    "mae_lstm = mean_absolute_error(y_test, lstm_predictions)\n",
    "rmse_attention = np.sqrt(mean_squared_error(y_test, attention_predictions))\n",
    "mae_attention = mean_absolute_error(y_test, attention_predictions)\n",
    "\n",
    "print(\"\\n--- Model Performance Evaluation ---\")\n",
    "print(f\"Baseline LSTM Model:\")\n",
    "print(f\"  RMSE: {rmse_lstm:.4f}\")\n",
    "print(f\"  MAE:  {mae_lstm:.4f}\")\n",
    "\n",
    "print(f\"\\nAttention-based Seq2Seq Model:\")\n",
    "print(f\"  RMSE: {rmse_attention:.4f}\")\n",
    "print(f\"  MAE:  {mae_attention:.4f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Generating forecast comparison plots...\")\n",
    "\n",
    "for i in range(n_features):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test[:, i], label='Actual Values', color='blue')\n",
    "    plt.plot(lstm_predictions[:, i], label='LSTM Predictions', color='red', linestyle='--')\n",
    "    plt.plot(attention_predictions[:, i], label='Attention Seq2Seq Predictions', color='green', linestyle=':')\n",
    "\n",
    "    plt.title(f'Feature {i+1}: Actual vs. Predicted Values')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Scaled Differenced Value')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Forecast comparison plots generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
